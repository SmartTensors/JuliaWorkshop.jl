{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Flux, Statistics, BSON, Zygote\n",
    "import Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "g (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g(x) = 5x^3 + 3x^2 + 2x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23.0,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Zygote.gradient(g, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g'(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g''(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.2870794464187259, -0.15072324287419367, 0.2545307787495331], [2.4980934937311376, -0.0852769368471369, 1.825895586034233])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ainp = rand(3)\n",
    "binp = rand(3)\n",
    "cinp = rand(3)\n",
    "\n",
    "function func(a, b)\n",
    "    temp = a.^2 + b.^2\n",
    "    out = (cinp - temp).^2\n",
    "    return sum(out)\n",
    "end\n",
    "\n",
    "Zygote.gradient(func, ainp, binp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-1.0573986293327713 0.0 … 0.0 0.0; 0.0 -0.15967164587968913 … -0.07338191150069401 0.0; … ; 0.0 -0.22889974206570568 … -0.1966533484846408 0.0; 0.0 0.0 … 0.0 -0.00013050951781236987], [-0.27720208189726325 -0.045438751583509136 … 0.0 0.0; -0.10620532917681685 -0.052254138639468796 … 0.0 0.0; … ; 0.0 0.0 … -0.10219294012069868 -0.08934439827810202; 0.0 0.0 … -5.3945807226079016e-5 -0.00010746416573991927])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ainp = rand(3,4)\n",
    "binp = rand(4,3)\n",
    "cinp = ones(3,3)\n",
    "\n",
    "function func(a, b)\n",
    "    temp = a * b\n",
    "    out = (cinp - temp).^2\n",
    "    return out\n",
    "end\n",
    "\n",
    "Zygote.jacobian(func, ainp, binp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "\n",
    "Consider solving `y = w * x + b`, which is standard gradient descent problem.\n",
    "\n",
    "The loss function is discrepency between estimated and true `y` (`yhat`)\n",
    "\n",
    "`L = RMS(yhat - y)`\n",
    "\n",
    "To solve it, we need to compute dL/dw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.1495067559506688 1.434866431317314 1.042899862082106; 0.013571459568988413 0.13024984480264554 0.09466912195874969; 0.14021211045434054 1.3456626041800532 0.9780640996806981; 0.0020561482589988318 0.01973354378461961 0.014342875157011598], [1.5022101214306587; 0.13636296097447465; 1.408819622446778; 0.020659713376760314;;])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Random\n",
    "Random.seed!(2022)\n",
    "\n",
    "# Inputs\n",
    "Wo = rand(4,3)\n",
    "bo = rand(4,1)\n",
    "x = rand(3)\n",
    "\n",
    "# Training data\n",
    "yhat = rand(4)\n",
    "\n",
    "# Loss function\n",
    "function loss(W, b)\n",
    "    y = W * x .+ b\n",
    "    l = 0.5 * (yhat - y) .^2\n",
    "    return sum(l)\n",
    "end\n",
    "\n",
    "\n",
    "Zygote.gradient(loss, Wo, bo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×1 Matrix{Float64}:\n",
       " 1.9145409176542763\n",
       " 0.5376768790526782\n",
       " 1.6628666613100092\n",
       " 1.008819269756692"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wo*x .+ bo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flux layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dense(3 => 4)       \u001b[90m# 16 parameters\u001b[39m"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linearlayer = Flux.Dense(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " -0.172069408331454\n",
       "  0.4817180636593049\n",
       "  0.6488648605809555\n",
       " -0.08067461133091389"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linearlayer(rand(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([Float32[-0.044647608 -0.31147394 0.3310886; -0.46983743 0.32885465 0.90849006; 0.16546322 0.552223 0.17763619; 0.17444198 0.09530168 -0.59155774], Float32[0.0, 0.0, 0.0, 0.0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = Flux.params(linearlayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×3 Matrix{Float32}:\n",
       " 0.426271  -0.41051     0.165463\n",
       " 0.375355  -0.467059    0.174442\n",
       " 0.82343   -0.0446476  -0.311474\n",
       " 0.225686  -0.469837    0.328855"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float32}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is: 2.317448061468025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Grads(...)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss(x)\n",
    "  y = linearlayer(x)\n",
    "  return sum((yhat .- y).^2)\n",
    "end\n",
    "\n",
    "l = loss(x)\n",
    "println(\"Loss is: \", l)\n",
    "\n",
    "gs = Zygote.gradient(()->loss(x), Flux.params(linearlayer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([Float32[-0.044647608 -0.31147394 0.3310886; -0.46983743 0.32885465 0.90849006; 0.16546322 0.552223 0.17763619; 0.17444198 0.09530168 -0.59155774], Float32[0.0, 0.0, 0.0, 0.0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Flux.params(linearlayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4×3 Matrix{Float32}:\n",
       "  0.156352   -0.110474   0.532089\n",
       " -0.670837    0.127855   0.70749\n",
       " -0.0355368   0.351223  -0.0233638\n",
       "  0.375442    0.296302  -0.390558"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss before: 0.5716848188579376\n",
      "after:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4×3 Matrix{Float32}:\n",
       "  0.256352  -0.010474    0.632089\n",
       " -0.770837   0.0278547   0.60749\n",
       " -0.135537   0.251223   -0.123364\n",
       "  0.475442   0.396302   -0.290558"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after: 0.5279925325805124\n",
      "before:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float32}:\n",
       "  0.20099999\n",
       " -0.20099999\n",
       " -0.20099999\n",
       "  0.20099999"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss before: 0.5279925325805124\n",
      "after:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float32}:\n",
       "  0.301\n",
       " -0.301\n",
       " -0.301\n",
       "  0.301"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after: 0.6129679051933241\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "opt = Flux.ADAM(0.1)\n",
    "\n",
    "for p in Flux.params(linearlayer)\n",
    "    println(\"before:\")\n",
    "    display(p)\n",
    "    println(\"loss before: \", loss(x))\n",
    "    Flux.Optimise.update!(opt, p, gs[p])\n",
    "    println(\"after:\")\n",
    "    display(p)\n",
    "    println(\"loss after: \", loss(x))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: Float32[0.4391527 -0.28688365 0.25531796; 0.38906926 -0.33544204 0.27010497; 0.83200884 0.037684325 -0.25163284; 0.24929641 -0.243239 0.49355254]\n",
      "loss before: 0.6700865544827526\n",
      "after: Float32[0.44584006 -0.22270271 0.3019664; 0.3961889 -0.2671124 0.31976882; 0.83646244 0.08042728 -0.2205661; 0.26155394 -0.1255995 0.57905614]\n",
      "loss after: 0.3465570819064103\n",
      "before: Float32[0.12942824, 0.13779455, 0.08619608, 0.23723355]\n",
      "loss before: 0.3465570819064103\n",
      "after: Float32[0.19662143, 0.20933115, 0.13094512, 0.3603943]\n",
      "loss after: 0.180602370202887\n"
     ]
    }
   ],
   "source": [
    "opt = Flux.Descent(0.1) # Gradient descent with learning rate 0.1\n",
    "\n",
    "for p in Flux.params(linearlayer)\n",
    "    println(\"before: \", p) \n",
    "    println(\"loss before: \", loss(x))\n",
    "    Flux.Optimise.update!(opt, p, gs[p])\n",
    "    println(\"after: \", p)\n",
    "    println(\"loss after: \", loss(x))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying backprop thru arbitrary layers\n",
    "## 1. FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "using FFTW\n",
    "Random.seed!(1234)\n",
    "# define problem\n",
    "x = rand(3);\n",
    "# Training data for Loss function definition\n",
    "yhat = rand(4);\n",
    "linearlayer = Dense(3,4,σ);\n",
    "model = Chain(linearlayer,x -> real(fft(x)));\n",
    "#model = Chain(linearlayer,x -> imag(fft(x)));\n",
    "out = model(x)\n",
    "println(out)\n",
    "\n",
    "# backprop thru this layer\n",
    "function loss(x)\n",
    "  y = model(x)\n",
    "  sum((y).^2)\n",
    "end\n",
    "\n",
    "l = loss(x)\n",
    "println(\"loss is: \", l)\n",
    "gs = gradient(() -> loss(x), params(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "using LinearAlgebra\n",
    "Random.seed!(1234)\n",
    "# define problem\n",
    "x = rand(3);\n",
    "xmat = Float32.(rand(10,10,1,1)); #conv input (w,l,nch,batch)\n",
    "yhatmat = Float32.(rand(10,10,1,1));\n",
    "# Training data for Loss function definition\n",
    "yhat = rand(4);\n",
    "linearlayer = Dense(3,4,σ);\n",
    "convlayer = Conv((3,3), 1 => 1, relu);\n",
    "#model = Chain(linearlayer,x -> svd(x));\n",
    "#model  = Chain(convlayer);\n",
    "model = Chain(convlayer,\n",
    "            x -> dropdims(x, dims=4),\n",
    "            x -> dropdims(x, dims=3),\n",
    "            x -> svd(x).S,\n",
    "            x -> x.^3 + 2*x);\n",
    "out = model(xmat);\n",
    "@show size(out)\n",
    "\n",
    "# backprop thru this layer\n",
    "function loss(x)\n",
    "  y = model(x)\n",
    "  sum((y).^2)\n",
    "end\n",
    "\n",
    "l = loss(xmat)\n",
    "#println(\"loss is: \", l)\n",
    "gs = gradient(() -> loss(xmat), params(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So adding SVD works.. although we have to drop dimensions and then add dimensions\n",
    "every iteration for it to work (since SVD only accepts a m x n matrix that can't have channel and nbatches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NNs with Arbitrary equations embedded\n",
    "Lets consider input X matrix.\n",
    "Define output Y which satisfies Y = X' + 2X.^2 \n",
    "Problem: We have Y, but need to learn X such that the equation is satisfied.\n",
    "\n",
    "This works, as amply demonstrated in documentation and my own experiments. Just find a good application problem to use it with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "using LinearAlgebra\n",
    "Random.seed!(1234)\n",
    "# define problem\n",
    "x = rand(3);\n",
    "dx = 0.1;\n",
    "\n",
    "xmat = Float32.(rand(10,10,1,1)); #conv input (w,l,nch,batch)\n",
    "yhatmat = Float32.(rand(10,10,1,1));\n",
    "# Training data for Loss function definition\n",
    "yhat = rand(4);\n",
    "\n",
    "linearlayer = Dense(3, 4, σ);\n",
    "convlayer = Conv((3, 3), 1=>1, pad=1, relu);\n",
    "\n",
    "function fdmlayer(x)\n",
    "    fdmgradx = (x[2:end,:] - x[1:end-1,:])/(2*dx);\n",
    "    fdmgrady = (x[:,2:end] - x[:,1:end-1])/(2*dx);\n",
    "    gradfull = fdmgradx' * fdmgrady'\n",
    "    #res = gradfull + 2 * x.^2\n",
    "    return gradfull\n",
    "end\n",
    "\n",
    "model = Chain(convlayer,\n",
    "            x -> dropdims(x, dims=4),\n",
    "            x -> dropdims(x, dims=3),\n",
    "            #x -> fdmlayer(x),\n",
    "            x -> x.^3 + 2*x);\n",
    "out = model(xmat);\n",
    "@show size(out)\n",
    "\n",
    "# backprop thru this layer\n",
    "function loss(x)\n",
    "  y = model(x)\n",
    "  sum((y).^2)\n",
    "end\n",
    "\n",
    "l = loss(xmat)\n",
    "#println(\"loss is: \", l)\n",
    "gs = gradient(() -> loss(xmat), params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdmgradx = (xmat[2:end,:] - xmat[1:end-1,:])/(2*dx);\n",
    "fdmgrady = (xmat[:,2:end] - xmat[:,1:end-1])/(2*dx);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = fdmgradx' * fdmgrady'\n",
    "size(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = map((x) -> x.^3 + 2*x, out);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((out2).^2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fabbfb9292bb03a077b67a9767abe5157b7697c4659fb99b687d3ca946780012"
  },
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
